{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ef370d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "from utils import (sentences, train_data, val_data, english_vectorizer, portuguese_vectorizer, \n",
    "                   masked_loss, masked_acc, tokens_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "226033a1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English (to translate) sentence:\n",
      "\n",
      "No matter how much you try to convince people that chocolate is vanilla, it'll still be chocolate, even though you may manage to convince yourself and a few others that it's vanilla.\n",
      "\n",
      "Portuguese (translation) sentence:\n",
      "\n",
      "Não importa o quanto você tenta convencer os outros de que chocolate é baunilha, ele ainda será chocolate, mesmo que você possa convencer a si mesmo e poucos outros de que é baunilha.\n"
     ]
    }
   ],
   "source": [
    "portuguese_sentences, english_sentences = sentences\n",
    "\n",
    "print(f\"English (to translate) sentence:\\n\\n{english_sentences[-5]}\\n\")\n",
    "print(f\"Portuguese (translation) sentence:\\n\\n{portuguese_sentences[-5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f081b0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "del portuguese_sentences\n",
    "del english_sentences\n",
    "del sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c1cfc17",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 words of the english vocabulary:\n",
      "\n",
      "['', '[UNK]', '[SOS]', '[EOS]', '.', 'tom', 'i', 'to', 'you', 'the']\n",
      "\n",
      "First 10 words of the portuguese vocabulary:\n",
      "\n",
      "['', '[UNK]', '[SOS]', '[EOS]', '.', 'tom', 'que', 'o', 'nao', 'eu']\n"
     ]
    }
   ],
   "source": [
    "print(f\"First 10 words of the english vocabulary:\\n\\n{english_vectorizer.get_vocabulary()[:10]}\\n\")\n",
    "print(f\"First 10 words of the portuguese vocabulary:\\n\\n{portuguese_vectorizer.get_vocabulary()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5facaa0c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portuguese vocabulary is made up of 12000 words\n",
      "English vocabulary is made up of 12000 words\n"
     ]
    }
   ],
   "source": [
    "# Size of the vocabulary\n",
    "vocab_size_por = portuguese_vectorizer.vocabulary_size()\n",
    "vocab_size_eng = english_vectorizer.vocabulary_size()\n",
    "\n",
    "print(f\"Portuguese vocabulary is made up of {vocab_size_por} words\")\n",
    "print(f\"English vocabulary is made up of {vocab_size_eng} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "218f7a36",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "word_to_id = tf.keras.layers.StringLookup(\n",
    "    vocabulary=portuguese_vectorizer.get_vocabulary(), \n",
    "    mask_token=\"\", \n",
    "    oov_token=\"[UNK]\"\n",
    ")\n",
    "\n",
    "id_to_word = tf.keras.layers.StringLookup(\n",
    "    vocabulary=portuguese_vectorizer.get_vocabulary(),\n",
    "    mask_token=\"\",\n",
    "    oov_token=\"[UNK]\",\n",
    "    invert=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20076b9a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The id for the [UNK] token is 1\n",
      "The id for the [SOS] token is 2\n",
      "The id for the [EOS] token is 3\n",
      "The id for baunilha (vanilla) is 7079\n"
     ]
    }
   ],
   "source": [
    "unk_id = word_to_id(\"[UNK]\")\n",
    "sos_id = word_to_id(\"[SOS]\")\n",
    "eos_id = word_to_id(\"[EOS]\")\n",
    "baunilha_id = word_to_id(\"baunilha\")\n",
    "\n",
    "print(f\"The id for the [UNK] token is {unk_id}\")\n",
    "print(f\"The id for the [SOS] token is {sos_id}\")\n",
    "print(f\"The id for the [EOS] token is {eos_id}\")\n",
    "print(f\"The id for baunilha (vanilla) is {baunilha_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "739777eb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized english sentence:\n",
      "[   2  210    9  146  123   38    9 1672    4    3    0    0    0    0]\n",
      "\n",
      "\n",
      "Tokenized portuguese sentence (shifted to the right):\n",
      "[   2 1085    7  128   11  389   37 2038    4    0    0    0    0    0\n",
      "    0]\n",
      "\n",
      "\n",
      "Tokenized portuguese sentence:\n",
      "[1085    7  128   11  389   37 2038    4    3    0    0    0    0    0\n",
      "    0]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (to_translate, sr_translation), translation in train_data.take(1):\n",
    "    print(f\"Tokenized english sentence:\\n{to_translate[0, :].numpy()}\\n\\n\")\n",
    "    print(f\"Tokenized portuguese sentence (shifted to the right):\\n{sr_translation[0, :].numpy()}\\n\\n\")\n",
    "    print(f\"Tokenized portuguese sentence:\\n{translation[0, :].numpy()}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e484abf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 12000\n",
    "UNITS = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1db0a1d",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, units):\n",
    "        \"\"\"Initializes an instance of this class\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary\n",
    "            units (int): Number of units in the LSTM layer\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(  \n",
    "            input_dim=vocab_size,\n",
    "            output_dim=units,\n",
    "            mask_zero=True\n",
    "        )  \n",
    "\n",
    "        self.rnn = tf.keras.layers.Bidirectional(  \n",
    "            merge_mode=\"sum\",  \n",
    "            layer=tf.keras.layers.LSTM(\n",
    "                units=units,\n",
    "                return_sequences=True\n",
    "            ),  \n",
    "        )  \n",
    "\n",
    "    def call(self, context):\n",
    "        \"\"\"Forward pass of this layer\n",
    "\n",
    "        Args:\n",
    "            context (tf.Tensor): The sentence to translate\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Encoded sentence to translate\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.embedding(context)\n",
    "        x = self.rnn(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65034ffd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of sentences in english has shape: (64, 14)\n",
      "\n",
      "Encoder output has shape: (64, 14, 256)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(VOCAB_SIZE, UNITS)\n",
    "\n",
    "encoder_output = encoder(to_translate)\n",
    "\n",
    "print(f'Tensor of sentences in english has shape: {to_translate.shape}\\n')\n",
    "print(f'Encoder output has shape: {encoder_output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74e71f3d",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        \"\"\"Initializes an instance of this class\n",
    "\n",
    "        Args:\n",
    "            units (int): Number of units in the LSTM layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = ( \n",
    "            tf.keras.layers.MultiHeadAttention(\n",
    "                key_dim=units,\n",
    "                num_heads=1\n",
    "            ) \n",
    "        )  \n",
    "\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, context, target):\n",
    "        \"\"\"Forward pass of this layer\n",
    "\n",
    "        Args:\n",
    "            context (tf.Tensor): Encoded sentence to translate\n",
    "            target (tf.Tensor): The embedded shifted-to-the-right translation\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Cross attention between context and target\n",
    "        \"\"\"\n",
    "\n",
    "        attn_output = self.mha(\n",
    "            query=target,\n",
    "            value=context\n",
    "        )  \n",
    "\n",
    "        x = self.add([target, attn_output])\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c62796f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of contexts has shape: (64, 14, 256)\n",
      "Tensor of translations has shape: (64, 15, 256)\n",
      "Tensor of attention scores has shape: (64, 15, 256)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = CrossAttention(UNITS)\n",
    "\n",
    "sr_translation_embed = tf.keras.layers.Embedding(VOCAB_SIZE, output_dim=UNITS, mask_zero=True)(sr_translation)\n",
    "\n",
    "attention_result = attention_layer(encoder_output, sr_translation_embed)\n",
    "\n",
    "print(f'Tensor of contexts has shape: {encoder_output.shape}')\n",
    "print(f'Tensor of translations has shape: {sr_translation_embed.shape}')\n",
    "print(f'Tensor of attention scores has shape: {attention_result.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9639bdb",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, units):\n",
    "        \"\"\"Initializes an instance of this class\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary\n",
    "            units (int): Number of units in the LSTM layer\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=units,\n",
    "            mask_zero=True\n",
    "        )  \n",
    "\n",
    "        self.pre_attention_rnn = tf.keras.layers.LSTM(\n",
    "            units=units,\n",
    "            return_sequences=True,\n",
    "            return_state=True\n",
    "        )  \n",
    "\n",
    "        self.attention = CrossAttention(units)\n",
    "\n",
    "        self.post_attention_rnn = tf.keras.layers.LSTM(\n",
    "            units=units,\n",
    "            return_sequences=True\n",
    "        )  \n",
    "\n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "            units=vocab_size,\n",
    "            activation=tf.nn.log_softmax\n",
    "        )  \n",
    "\n",
    "\n",
    "    def call(self, context, target, state=None, return_state=False):\n",
    "        \"\"\"Forward pass of this layer\n",
    "\n",
    "        Args:\n",
    "            context (tf.Tensor): Encoded sentence to translate\n",
    "            target (tf.Tensor): The shifted-to-the-right translation\n",
    "            state (list[tf.Tensor, tf.Tensor], optional): Hidden state of the pre-attention LSTM. Defaults to None.\n",
    "            return_state (bool, optional): If set to true return the hidden states of the LSTM. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The log_softmax probabilities of predicting a particular token\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.embedding(target)\n",
    "\n",
    "        x, hidden_state, cell_state = self.pre_attention_rnn(x, initial_state=state)\n",
    "\n",
    "        x = self.attention(context, x)\n",
    "        x = self.post_attention_rnn(x)\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        if return_state:\n",
    "            return logits, [hidden_state, cell_state]\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6165cf2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of contexts has shape: (64, 14, 256)\n",
      "Tensor of right-shifted translations has shape: (64, 15)\n",
      "Tensor of logits has shape: (64, 15, 12000)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(VOCAB_SIZE, UNITS)\n",
    "\n",
    "logits = decoder(encoder_output, sr_translation)\n",
    "\n",
    "print(f'Tensor of contexts has shape: {encoder_output.shape}')\n",
    "print(f'Tensor of right-shifted translations has shape: {sr_translation.shape}')\n",
    "print(f'Tensor of logits has shape: {logits.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "205fcf31",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "class Translator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, units):\n",
    "        \"\"\"Initializes an instance of this class\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary\n",
    "            units (int): Number of units in the LSTM layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(vocab_size=vocab_size, units=units)\n",
    "\n",
    "        self.decoder = Decoder(vocab_size=vocab_size, units=units)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Forward pass of this layer\n",
    "\n",
    "        Args:\n",
    "            inputs (tuple(tf.Tensor, tf.Tensor)): Tuple containing the context (sentence to translate) and the target (shifted-to-the-right translation)\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The log_softmax probabilities of predicting a particular token\n",
    "        \"\"\"\n",
    "\n",
    "        context, target = inputs\n",
    "\n",
    "        encoded_context = self.encoder(context)\n",
    "\n",
    "        logits = self.decoder(encoded_context, target)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d4a231c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of sentences to translate has shape: (64, 14)\n",
      "Tensor of right-shifted translations has shape: (64, 15)\n",
      "Tensor of logits has shape: (64, 15, 12000)\n"
     ]
    }
   ],
   "source": [
    "translator = Translator(VOCAB_SIZE, UNITS)\n",
    "\n",
    "logits = translator((to_translate, sr_translation))\n",
    "\n",
    "print(f'Tensor of sentences to translate has shape: {to_translate.shape}')\n",
    "print(f'Tensor of right-shifted translations has shape: {sr_translation.shape}')\n",
    "print(f'Tensor of logits has shape: {logits.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a61ef65",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def compile_and_train(model, epochs=20, steps_per_epoch=500):\n",
    "    model.compile(optimizer=\"adam\", loss=masked_loss, metrics=[masked_acc, masked_loss])\n",
    "\n",
    "    history = model.fit(\n",
    "        train_data.repeat(),\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=val_data,\n",
    "        validation_steps=50,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)],\n",
    "    )\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87d9bf9f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "500/500 [==============================] - 48s 68ms/step - loss: 5.2155 - masked_acc: 0.2086 - masked_loss: 5.2180 - val_loss: 4.4276 - val_masked_acc: 0.3108 - val_masked_loss: 4.4285\n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 16s 33ms/step - loss: 3.7767 - masked_acc: 0.4116 - masked_loss: 3.7775 - val_loss: 3.0601 - val_masked_acc: 0.5024 - val_masked_loss: 3.0619\n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 16s 32ms/step - loss: 2.7577 - masked_acc: 0.5430 - masked_loss: 2.7592 - val_loss: 2.4093 - val_masked_acc: 0.5839 - val_masked_loss: 2.4096\n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 16s 32ms/step - loss: 2.2430 - masked_acc: 0.6117 - masked_loss: 2.2439 - val_loss: 1.9976 - val_masked_acc: 0.6437 - val_masked_loss: 1.9979\n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 15s 31ms/step - loss: 1.8893 - masked_acc: 0.6623 - masked_loss: 1.8906 - val_loss: 1.7364 - val_masked_acc: 0.6779 - val_masked_loss: 1.7371\n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 15s 30ms/step - loss: 1.6322 - masked_acc: 0.6964 - masked_loss: 1.6332 - val_loss: 1.6126 - val_masked_acc: 0.7011 - val_masked_loss: 1.6117\n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 15s 31ms/step - loss: 1.5191 - masked_acc: 0.7127 - masked_loss: 1.5198 - val_loss: 1.4999 - val_masked_acc: 0.7109 - val_masked_loss: 1.5002\n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 16s 32ms/step - loss: 1.4148 - masked_acc: 0.7269 - masked_loss: 1.4158 - val_loss: 1.3974 - val_masked_acc: 0.7240 - val_masked_loss: 1.3991\n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 16s 31ms/step - loss: 1.3378 - masked_acc: 0.7353 - masked_loss: 1.3392 - val_loss: 1.3226 - val_masked_acc: 0.7346 - val_masked_loss: 1.3228\n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 15s 31ms/step - loss: 1.2079 - masked_acc: 0.7525 - masked_loss: 1.2093 - val_loss: 1.2425 - val_masked_acc: 0.7468 - val_masked_loss: 1.2434\n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 15s 30ms/step - loss: 1.1047 - masked_acc: 0.7642 - masked_loss: 1.1056 - val_loss: 1.1939 - val_masked_acc: 0.7566 - val_masked_loss: 1.1939\n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 15s 30ms/step - loss: 1.0834 - masked_acc: 0.7689 - masked_loss: 1.0843 - val_loss: 1.1464 - val_masked_acc: 0.7616 - val_masked_loss: 1.1464\n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 15s 31ms/step - loss: 1.0577 - masked_acc: 0.7708 - masked_loss: 1.0590 - val_loss: 1.1601 - val_masked_acc: 0.7561 - val_masked_loss: 1.1610\n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 15s 30ms/step - loss: 1.0313 - masked_acc: 0.7760 - masked_loss: 1.0325 - val_loss: 1.1080 - val_masked_acc: 0.7659 - val_masked_loss: 1.1086\n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 16s 31ms/step - loss: 0.9251 - masked_acc: 0.7890 - masked_loss: 0.9262 - val_loss: 1.0891 - val_masked_acc: 0.7662 - val_masked_loss: 1.0904\n",
      "Epoch 16/20\n",
      "500/500 [==============================] - 15s 30ms/step - loss: 0.8757 - masked_acc: 0.7959 - masked_loss: 0.8764 - val_loss: 1.0915 - val_masked_acc: 0.7687 - val_masked_loss: 1.0931\n",
      "Epoch 17/20\n",
      "500/500 [==============================] - 15s 30ms/step - loss: 0.8869 - masked_acc: 0.7938 - masked_loss: 0.8876 - val_loss: 1.0123 - val_masked_acc: 0.7774 - val_masked_loss: 1.0136\n",
      "Epoch 18/20\n",
      "500/500 [==============================] - 15s 31ms/step - loss: 0.8818 - masked_acc: 0.7946 - masked_loss: 0.8826 - val_loss: 1.0144 - val_masked_acc: 0.7793 - val_masked_loss: 1.0157\n",
      "Epoch 19/20\n",
      "500/500 [==============================] - 15s 30ms/step - loss: 0.8647 - masked_acc: 0.7973 - masked_loss: 0.8654 - val_loss: 1.0088 - val_masked_acc: 0.7781 - val_masked_loss: 1.0085\n",
      "Epoch 20/20\n",
      "500/500 [==============================] - 15s 31ms/step - loss: 0.7547 - masked_acc: 0.8148 - masked_loss: 0.7556 - val_loss: 0.9980 - val_masked_acc: 0.7816 - val_masked_loss: 0.9986\n"
     ]
    }
   ],
   "source": [
    "trained_translator, history = compile_and_train(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "522f6b6f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def generate_next_token(decoder, context, next_token, done, state, temperature=0.0):\n",
    "    \"\"\"Generates the next token in the sequence\n",
    "\n",
    "    Args:\n",
    "        decoder (Decoder): The decoder\n",
    "        context (tf.Tensor): Encoded sentence to translate\n",
    "        next_token (tf.Tensor): The predicted next token\n",
    "        done (bool): True if the translation is complete\n",
    "        state (list[tf.Tensor, tf.Tensor]): Hidden states of the pre-attention LSTM layer\n",
    "        temperature (float, optional): The temperature that controls the randomness of the predicted tokens. Defaults to 0.0.\n",
    "\n",
    "    Returns:\n",
    "        tuple(tf.Tensor, np.float, list[tf.Tensor, tf.Tensor], bool): The next token, log prob of said token, hidden state of LSTM and if translation is done\n",
    "    \"\"\"\n",
    "    \n",
    "    logits, state = decoder(context, next_token, state=state, return_state=True)\n",
    "    \n",
    "    logits = logits[:, -1, :]\n",
    "\n",
    "    if temperature == 0.0:\n",
    "        next_token = tf.argmax(logits, axis=-1)\n",
    "\n",
    "    else:\n",
    "        logits = logits / temperature\n",
    "        next_token = tf.random.categorical(logits, num_samples=1)\n",
    "\n",
    "    logits = tf.squeeze(logits)\n",
    "    next_token = tf.squeeze(next_token)\n",
    "    \n",
    "    logit = logits[next_token].numpy()\n",
    "\n",
    "    next_token = tf.reshape(next_token, shape=(1,1))\n",
    "\n",
    "    if next_token == eos_id:\n",
    "        done = True\n",
    "    \n",
    "    return next_token, logit, state, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9937547a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token: [[6188]]\n",
      "Logit: -18.7799\n",
      "Done? False\n"
     ]
    }
   ],
   "source": [
    "# PROCESS SENTENCE TO TRANSLATE AND ENCODE\n",
    "\n",
    "eng_sentence = \"I love languages\"\n",
    "\n",
    "texts = tf.convert_to_tensor(eng_sentence)[tf.newaxis]\n",
    "\n",
    "context = english_vectorizer(texts).to_tensor()\n",
    "context = encoder(context)\n",
    "\n",
    "next_token = tf.fill((1,1), sos_id)\n",
    "\n",
    "state = [tf.random.uniform((1, UNITS)), tf.random.uniform((1, UNITS))]\n",
    "\n",
    "done = False\n",
    "\n",
    "next_token, logit, state, done = generate_next_token(decoder, context, next_token, done, state, temperature=0.5)\n",
    "print(f\"Next token: {next_token}\\nLogit: {logit:.4f}\\nDone? {done}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42c74f1f",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def translate(model, text, max_length=50, temperature=0.0):\n",
    "    \"\"\"Translate a given sentence from English to Portuguese\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The trained translator\n",
    "        text (string): The sentence to translate\n",
    "        max_length (int, optional): The maximum length of the translation. Defaults to 50.\n",
    "        temperature (float, optional): The temperature that controls the randomness of the predicted tokens. Defaults to 0.0.\n",
    "\n",
    "    Returns:\n",
    "        tuple(str, np.float, tf.Tensor): The translation, logit that predicted <EOS> token and the tokenized translation\n",
    "    \"\"\"\n",
    "\n",
    "    tokens, logits = [], []\n",
    "\n",
    "    text = tf.convert_to_tensor(text)[tf.newaxis]\n",
    "    \n",
    "    context = english_vectorizer(text).to_tensor()\n",
    "    context = model.encoder(context)\n",
    "\n",
    "    next_token = tf.fill((1, 1), sos_id)\n",
    "\n",
    "    state = [tf.zeros((1, UNITS)), tf.zeros((1, UNITS))]\n",
    "\n",
    "    done = False\n",
    "\n",
    "    for i in range(max_length):\n",
    "        try:\n",
    "            next_token, logit, state, done = generate_next_token(\n",
    "                decoder=model.decoder,\n",
    "                context=context,\n",
    "                next_token=next_token,\n",
    "                done=done,\n",
    "                state=state,\n",
    "                temperature=temperature\n",
    "            )\n",
    "        except:\n",
    "             raise Exception(\"Problem generating the next token\")\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        tokens.append(next_token)\n",
    "\n",
    "        logits.append(logit)\n",
    "    \n",
    "    tokens = tf.concat(tokens, axis=-1)\n",
    "\n",
    "    translation = tf.squeeze(tokens_to_text(tokens, id_to_word))\n",
    "    translation = translation.numpy().decode()\n",
    "    \n",
    "    return translation, logits[-1], tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "daaea8c5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.0\n",
      "\n",
      "Original sentence: I love languages\n",
      "Translation: eu amo idiomas de vez em quando eu amo . eu eu amo eu eu amo eu eu amo eu amo eu amo eu amo .\n",
      "Translation tokens:[[  9 522 850  11 135  22  60   9 522   4   9   9 522   9   9 522   9   9\n",
      "  522   9 522   9 522   9 522   4]]\n",
      "Logit: -1.248\n"
     ]
    }
   ],
   "source": [
    "temp = 0.0 \n",
    "original_sentence = \"I love languages\"\n",
    "\n",
    "translation, logit, tokens = translate(trained_translator, original_sentence, temperature=temp)\n",
    "\n",
    "print(f\"Temperature: {temp}\\n\\nOriginal sentence: {original_sentence}\\nTranslation: {translation}\\nTranslation tokens:{tokens}\\nLogit: {logit:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e0697db",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.7\n",
      "\n",
      "Original sentence: I love languages\n",
      "Translation: eu amo idiomas dele [UNK] .\n",
      "Translation tokens:[[  9 522 850 115   1   4]]\n",
      "Logit: -0.579\n"
     ]
    }
   ],
   "source": [
    "temp = 0.7\n",
    "original_sentence = \"I love languages\"\n",
    "\n",
    "translation, logit, tokens = translate(trained_translator, original_sentence, temperature=temp)\n",
    "\n",
    "print(f\"Temperature: {temp}\\n\\nOriginal sentence: {original_sentence}\\nTranslation: {translation}\\nTranslation tokens:{tokens}\\nLogit: {logit:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62301cd5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def generate_samples(model, text, n_samples=4, temperature=0.6):\n",
    "    \n",
    "    samples, log_probs = [], []\n",
    "\n",
    "    # Iterate for n_samples iterations\n",
    "    for _ in range(n_samples):\n",
    "        \n",
    "        # Save the logit and the translated tensor\n",
    "        _, logp, sample = translate(model, text, temperature=temperature)\n",
    "        \n",
    "        # Save the translated tensors\n",
    "        samples.append(np.squeeze(sample.numpy()).tolist())\n",
    "        \n",
    "        # Save the logits\n",
    "        log_probs.append(logp)\n",
    "                \n",
    "    return samples, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06bd792c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated tensor: [9, 522, 850, 305, 4] has logit: -0.613\n",
      "Translated tensor: [9, 522, 850, 11, 135, 22, 60, 9, 564, 4, 9, 9, 9, 522, 9, 9, 522, 9, 9, 1270, 564, 9, 522, 9, 522, 89, 9, 9, 522, 1, 4] has logit: -0.482\n",
      "Translated tensor: [9, 564, 850, 4] has logit: -2.231\n",
      "Translated tensor: [9, 522, 850, 41, 877, 135, 4] has logit: -1.064\n"
     ]
    }
   ],
   "source": [
    "samples, log_probs = generate_samples(trained_translator, 'I love languages')\n",
    "\n",
    "for s, l in zip(samples, log_probs):\n",
    "    print(f\"Translated tensor: {s} has logit: {l:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "edb54a71",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def jaccard_similarity(candidate, reference):\n",
    "        \n",
    "    # Convert the lists to sets to get the unique tokens\n",
    "    candidate_set = set(candidate)\n",
    "    reference_set = set(reference)\n",
    "    \n",
    "    # Get the set of tokens common to both candidate and reference\n",
    "    common_tokens = candidate_set.intersection(reference_set)\n",
    "    \n",
    "    # Get the set of all tokens found in either candidate or reference\n",
    "    all_tokens = candidate_set.union(reference_set)\n",
    "    \n",
    "    # Compute the percentage of overlap (divide the number of common tokens by the number of all tokens)\n",
    "    overlap = len(common_tokens) / len(all_tokens)\n",
    "        \n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc3384bf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard similarity between lists: [1, 2, 3] and [1, 2, 3, 4] is 0.750\n"
     ]
    }
   ],
   "source": [
    "l1 = [1, 2, 3]\n",
    "l2 = [1, 2, 3, 4]\n",
    "\n",
    "js = jaccard_similarity(l1, l2)\n",
    "\n",
    "print(f\"jaccard similarity between lists: {l1} and {l2} is {js:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb2e0a00",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def rouge1_similarity(candidate, reference):\n",
    "    \"\"\"Computes the ROUGE 1 score between two token lists\n",
    "\n",
    "    Args:\n",
    "        candidate (list[int]): Tokenized candidate translation\n",
    "        reference (list[int]): Tokenized reference translation\n",
    "\n",
    "    Returns:\n",
    "        float: Overlap between the two token lists\n",
    "    \"\"\"\n",
    "    candidate_word_counts = Counter(candidate)\n",
    "    reference_word_counts = Counter(reference)\n",
    "    \n",
    "    overlap = 0\n",
    "    \n",
    "    for token in candidate_word_counts.keys():\n",
    "        \n",
    "        token_count_candidate = candidate_word_counts[token]\n",
    "        \n",
    "        token_count_reference = reference_word_counts[token]\n",
    "        \n",
    "        overlap += min(token_count_candidate, token_count_reference)\n",
    "    \n",
    "    precision = overlap / len(candidate)\n",
    "    \n",
    "    recall = overlap / len(reference)\n",
    "    \n",
    "    if precision + recall != 0:\n",
    "        f1_score = 2 * precision * recall / (precision + recall)\n",
    "        \n",
    "        return f1_score\n",
    "    \n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "14bb5295",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge 1 similarity between lists: [1, 2, 3] and [1, 2, 3, 4] is 0.857\n"
     ]
    }
   ],
   "source": [
    "l1 = [1, 2, 3]\n",
    "l2 = [1, 2, 3, 4]\n",
    "\n",
    "r1s = rouge1_similarity(l1, l2)\n",
    "\n",
    "print(f\"rouge 1 similarity between lists: {l1} and {l2} is {r1s:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "142264ff",
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def average_overlap(samples, similarity_fn):\n",
    "    \"\"\"Computes the arithmetic mean of each candidate sentence in the samples\n",
    "\n",
    "    Args:\n",
    "        samples (list[list[int]]): Tokenized version of translated sentences\n",
    "        similarity_fn (Function): Similarity function used to compute the overlap\n",
    "\n",
    "    Returns:\n",
    "        dict[int, float]: A dictionary mapping the index of each translation to its score\n",
    "    \"\"\"\n",
    "\n",
    "    scores = {}\n",
    "    \n",
    "    for index_candidate, candidate in enumerate(samples):    \n",
    "        \n",
    "        overlap = 0\n",
    "        \n",
    "        for index_sample, sample in enumerate(samples):\n",
    "\n",
    "            if index_candidate == index_sample:\n",
    "                continue\n",
    "                \n",
    "            sample_overlap = similarity_fn(candidate, sample)\n",
    "            \n",
    "            overlap += sample_overlap\n",
    "\n",
    "\n",
    "        score = overlap / (len(samples) - 1)\n",
    "\n",
    "        score = round(score, 3)\n",
    "        \n",
    "        scores[index_candidate] = score\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f36cf403",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average overlap between lists: [1, 2, 3], [1, 2, 4] and [1, 2, 4, 5] using Jaccard similarity is:\n",
      "\n",
      "{0: 0.45, 1: 0.625, 2: 0.575}\n"
     ]
    }
   ],
   "source": [
    "# Test with Jaccard similarity\n",
    "\n",
    "l1 = [1, 2, 3]\n",
    "l2 = [1, 2, 4]\n",
    "l3 = [1, 2, 4, 5]\n",
    "\n",
    "avg_ovlp = average_overlap([l1, l2, l3], jaccard_similarity)\n",
    "\n",
    "print(f\"average overlap between lists: {l1}, {l2} and {l3} using Jaccard similarity is:\\n\\n{avg_ovlp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d961a304-7c03-4ecb-ba5f-c8747ed3ec39",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average overlap between lists: [1, 2, 3], [1, 4], [1, 2, 4, 5] and [5, 6] using Rouge1 similarity is:\n",
      "\n",
      "{0: 0.324, 1: 0.356, 2: 0.524, 3: 0.111}\n"
     ]
    }
   ],
   "source": [
    "# Test with Rouge1 similarity\n",
    "\n",
    "l1 = [1, 2, 3]\n",
    "l2 = [1, 4]\n",
    "l3 = [1, 2, 4, 5]\n",
    "l4 = [5,6]\n",
    "\n",
    "avg_ovlp = average_overlap([l1, l2, l3, l4], rouge1_similarity)\n",
    "\n",
    "print(f\"average overlap between lists: {l1}, {l2}, {l3} and {l4} using Rouge1 similarity is:\\n\\n{avg_ovlp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "398714be",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def weighted_avg_overlap(samples, log_probs, similarity_fn):\n",
    "    \n",
    "    scores = {}\n",
    "    \n",
    "    for index_candidate, candidate in enumerate(samples):    \n",
    "        \n",
    "        overlap, weight_sum = 0.0, 0.0\n",
    "\n",
    "        for index_sample, (sample, logp) in enumerate(zip(samples, log_probs)):\n",
    "         \n",
    "            if index_candidate == index_sample:\n",
    "                continue\n",
    "                \n",
    "            sample_p = float(np.exp(logp))\n",
    "\n",
    "            weight_sum += sample_p\n",
    "\n",
    "            sample_overlap = similarity_fn(candidate, sample)\n",
    "            \n",
    "            overlap += sample_p * sample_overlap\n",
    "            \n",
    "        score = overlap / weight_sum\n",
    "\n",
    "        score = round(score, 3)\n",
    "        \n",
    "        scores[index_candidate] = score\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e3dfd6d3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted average overlap using Jaccard similarity is:\n",
      "\n",
      "{0: 0.443, 1: 0.631, 2: 0.558}\n"
     ]
    }
   ],
   "source": [
    "l1 = [1, 2, 3]\n",
    "l2 = [1, 2, 4]\n",
    "l3 = [1, 2, 4, 5]\n",
    "log_probs = [0.4, 0.2, 0.5]\n",
    "\n",
    "w_avg_ovlp = weighted_avg_overlap([l1, l2, l3], log_probs, jaccard_similarity)\n",
    "\n",
    "print(f\"weighted average overlap using Jaccard similarity is:\\n\\n{w_avg_ovlp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6fcfa640",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def mbr_decode(model, text, n_samples=5, temperature=0.6, similarity_fn=jaccard_similarity):\n",
    "    \n",
    "    # Generate samples\n",
    "    samples, log_probs = generate_samples(model, text, n_samples=n_samples, temperature=temperature)\n",
    "    \n",
    "    # Compute the overlap scores\n",
    "    scores = weighted_avg_overlap(samples, log_probs, similarity_fn)\n",
    "\n",
    "    # Decode samples\n",
    "    decoded_translations = [tokens_to_text(s, id_to_word).numpy().decode('utf-8') for s in samples]\n",
    "    \n",
    "    # Find the key with the highest score\n",
    "    max_score_key = max(scores, key=lambda k: scores[k])\n",
    "    \n",
    "    # Get the translation \n",
    "    translation = decoded_translations[max_score_key]\n",
    "    \n",
    "    return translation, decoded_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99507fcc-7727-45e7-933b-d3a08034f731",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation candidates:\n",
      "eu amo idiomas desse redor de vez solar .\n",
      "eu amo idiomas a senhora .\n",
      "eu adoro idiomas dele tem nenhum pouco . eu respeito . eu adoro eu eu amo eu eu negocios eu adoro negocios .\n",
      "eu amo idiomas .\n",
      "eu adoro idiomas dela .\n",
      "eu amo idiomas dele ?\n",
      "eu amo idiomas algum propria vez .\n",
      "eu amo idiomas do mundo .\n",
      "eu amo idiomas tem um monte de mesa .\n",
      "eu adoro idiomas .\n",
      "\n",
      "Selected translation: eu amo idiomas .\n"
     ]
    }
   ],
   "source": [
    "english_sentence = \"I love languages\"\n",
    "\n",
    "translation, candidates = mbr_decode(trained_translator, english_sentence, n_samples=10, temperature=0.6)\n",
    "\n",
    "print(\"Translation candidates:\")\n",
    "for c in candidates:\n",
    "    print(c)\n",
    "\n",
    "print(f\"\\nSelected translation: {translation}\")"
   ]
  }
 ],
 "metadata": {
  "grader_version": "1",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
